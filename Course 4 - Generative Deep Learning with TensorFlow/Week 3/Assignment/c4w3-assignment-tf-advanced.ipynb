{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Week 3: Variational Autoencoders on Anime Faces","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"## Imports","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nimport tensorflow_datasets as tfds\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nimport os\nimport zipfile\nimport urllib.request\nimport random\nfrom IPython import display","metadata":{"execution":{"iopub.status.busy":"2024-07-30T13:47:54.870507Z","iopub.execute_input":"2024-07-30T13:47:54.871357Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"2024-07-30 13:47:56.809042: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-07-30 13:47:56.809223: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-07-30 13:47:56.954724: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Parameters","metadata":{}},{"cell_type":"code","source":"# set a random seed\nnp.random.seed(51)\n\n# parameters for building the model and training\nBATCH_SIZE=2000\nLATENT_DIM=512\nIMAGE_SIZE=64","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Download the Dataset","metadata":{}},{"cell_type":"code","source":"# make the data directory\ntry:\n  os.mkdir('/tmp/anime')\nexcept OSError:\n  pass\n\n# download the zipped dataset to the data directory\ndata_url = \"https://storage.googleapis.com/learning-datasets/Resources/anime-faces.zip\"\ndata_file_name = \"animefaces.zip\"\ndownload_dir = '/tmp/anime/'\nurllib.request.urlretrieve(data_url, data_file_name)\n\n# extract the zip file\nzip_ref = zipfile.ZipFile(data_file_name, 'r')\nzip_ref.extractall(download_dir)\nzip_ref.close()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Prepare the Dataset","metadata":{}},{"cell_type":"code","source":"# Data Preparation Utilities\n\ndef get_dataset_slice_paths(image_dir):\n  '''returns a list of paths to the image files'''\n  image_file_list = os.listdir(image_dir)\n  image_paths = [os.path.join(image_dir, fname) for fname in image_file_list]\n\n  return image_paths","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def map_image(image_filename):\n  '''preprocesses the images'''\n  img_raw = tf.io.read_file(image_filename)\n  image = tf.image.decode_jpeg(img_raw)\n\n  image = tf.cast(image, dtype=tf.float32)\n  image = tf.image.resize(image, (IMAGE_SIZE, IMAGE_SIZE))\n  image = image / 255.0  \n  image = tf.reshape(image, shape=(IMAGE_SIZE, IMAGE_SIZE, 3,))\n\n  return image","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get the list containing the image paths\npaths = get_dataset_slice_paths(\"/tmp/anime/images/\")\n\n# shuffle the paths\nrandom.shuffle(paths)\n\n# split the paths list into to training (80%) and validation sets(20%).\npaths_len = len(paths)\ntrain_paths_len = int(paths_len * 0.8)\n\ntrain_paths = paths[:train_paths_len]\nval_paths = paths[train_paths_len:]\n\n# load the training image paths into tensors, create batches and shuffle\ntraining_dataset = tf.data.Dataset.from_tensor_slices((train_paths))\ntraining_dataset = training_dataset.map(map_image)\ntraining_dataset = training_dataset.shuffle(1000).batch(BATCH_SIZE)\n\n# load the validation image paths into tensors and create batches\nvalidation_dataset = tf.data.Dataset.from_tensor_slices((val_paths))\nvalidation_dataset = validation_dataset.map(map_image)\nvalidation_dataset = validation_dataset.batch(BATCH_SIZE)\n\n\nprint(f'number of batches in the training set: {len(training_dataset)}')\nprint(f'number of batches in the validation set: {len(validation_dataset)}')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Display Utilities","metadata":{}},{"cell_type":"code","source":"def display_faces(dataset, size=9):\n  '''Takes a sample from a dataset batch and plots it in a grid.'''\n  dataset = dataset.unbatch().take(size)\n  n_cols = 3\n  n_rows = size//n_cols + 1\n  plt.figure(figsize=(5, 5))\n  i = 0\n  for image in dataset:\n    i += 1\n    disp_img = np.reshape(image, (64,64,3))\n    plt.subplot(n_rows, n_cols, i)\n    plt.xticks([])\n    plt.yticks([])\n    plt.imshow(disp_img)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def display_one_row(disp_images, offset, shape=(28, 28)):\n  '''Displays a row of images.'''\n  for idx, image in enumerate(disp_images):\n    plt.subplot(3, 10, offset + idx + 1)\n    plt.xticks([])\n    plt.yticks([])\n    image = np.reshape(image, shape)\n    plt.imshow(image)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def display_results(disp_input_images, disp_predicted):\n  '''Displays input and predicted images.'''\n  plt.figure(figsize=(15, 5))\n  display_one_row(disp_input_images, 0, shape=(IMAGE_SIZE,IMAGE_SIZE,3))\n  display_one_row(disp_predicted, 20, shape=(IMAGE_SIZE,IMAGE_SIZE,3))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display_faces(validation_dataset, size=12)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Build the Model","metadata":{}},{"cell_type":"markdown","source":"### Sampling Class","metadata":{}},{"cell_type":"code","source":"class Sampling(tf.keras.layers.Layer):\n  def call(self, inputs):\n    \"\"\"Generates a random sample and combines with the encoder output\n    \n    Args:\n      inputs -- output tensor from the encoder\n\n    Returns:\n      `inputs` tensors combined with a random sample\n    \"\"\"\n    ### START CODE HERE ###\n    mu, sigma = inputs\n    batch = tf.shape(mu)[0]\n    dim = tf.shape(mu)[1]\n    epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n    z = mu + tf.exp(0.5 * sigma) * epsilon\n    ### END CODE HERE ###\n    return  z","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Encoder Layers","metadata":{}},{"cell_type":"code","source":"# Function to define the layers of the encoder\ndef encoder_layers(inputs, latent_dim):\n    x = tf.keras.layers.Conv2D(filters=32, kernel_size=3, strides=2, padding=\"same\", activation='relu', name=\"encode_conv1\")(inputs)  # First convolutional layer\n    x = tf.keras.layers.BatchNormalization()(x)  # Batch normalization\n    x = tf.keras.layers.Conv2D(filters=64, kernel_size=3, strides=2, padding='same', activation='relu', name=\"encode_conv2\")(x)  # Second convolutional layer\n    batch_2 = tf.keras.layers.BatchNormalization()(x)  # Batch normalization\n    x = tf.keras.layers.Flatten(name=\"encode_flatten\")(batch_2)  # Flatten the feature maps\n    x = tf.keras.layers.Dense(20, activation='relu', name=\"encode_dense\")(x)  # Dense layer with 20 units\n    x = tf.keras.layers.BatchNormalization()(x)  # Batch normalization\n    mu = tf.keras.layers.Dense(latent_dim, name='latent_mu')(x)  # Dense layer to output the mean of the latent space\n    sigma = tf.keras.layers.Dense(latent_dim, name ='latent_sigma')(x)  # Dense layer to output the log variance of the latent space\n    return mu, sigma, batch_2.shape  # Return the mean, log variance, and shape of the feature maps","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Encoder Model","metadata":{}},{"cell_type":"code","source":"# Function to build the encoder model\ndef encoder_model(latent_dim, input_shape):\n    inputs = tf.keras.layers.Input(shape=input_shape)  # Input layer with the given shape\n    mu, sigma, conv_shape = encoder_layers(inputs, latent_dim=latent_dim)  # Get the outputs of the encoder layers\n    z = Sampling()((mu, sigma))  # Sample z using the Sampling layer\n    model = tf.keras.Model(inputs, outputs=[mu, sigma, z])  # Define the encoder model\n    return model, conv_shape  # Return the encoder model and the shape of the feature maps","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Decoder Layers","metadata":{}},{"cell_type":"code","source":"# Function to define the layers of the decoder\ndef decoder_layers(inputs, conv_shape):\n    units = conv_shape[1] * conv_shape[2] * conv_shape[3]  # Compute the number of units for the Dense layer\n    x = tf.keras.layers.Dense(units, activation='relu', name=\"decode_dense1\")(inputs)  # Dense layer to expand the latent space\n    x = tf.keras.layers.BatchNormalization()(x)  # Batch normalization\n    x = tf.keras.layers.Reshape((conv_shape[1], conv_shape[2], conv_shape[3]), name=\"decode_reshape\")(x)  # Reshape to the shape of the feature maps\n    x = tf.keras.layers.Conv2DTranspose(filters=64, kernel_size=3, strides=2, padding='same', activation='relu', name=\"decode_conv2d_2\")(x)  # First transposed convolutional layer\n    x = tf.keras.layers.BatchNormalization()(x)  # Batch normalization\n    x = tf.keras.layers.Conv2DTranspose(filters=32, kernel_size=3, strides=2, padding='same', activation='relu', name=\"decode_conv2d_3\")(x)  # Second transposed convolutional layer\n    x = tf.keras.layers.BatchNormalization()(x)  # Batch normalization\n    x = tf.keras.layers.Conv2DTranspose(filters=1, kernel_size=3, strides=1, padding='same', activation='sigmoid', name=\"decode_final\")(x)  # Final transposed convolutional layer\n    return x  # Return the output of the decoder","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Decoder Model","metadata":{}},{"cell_type":"code","source":"def decoder_model(latent_dim, conv_shape):\n  \"\"\"Defines the decoder model.\n  Args:\n    latent_dim -- dimensionality of the latent space\n    conv_shape -- shape of the features before flattening\n\n  Returns:\n    model -- the decoder model\n  \"\"\"\n  ### START CODE HERE ###\n  inputs = tf.keras.layers.Input(shape=(latent_dim,))\n  outputs = decoder_layers(inputs, conv_shape)\n  model = tf.keras.Model(inputs, outputs)\n  ### END CODE HERE ###\n  model.summary()\n  return model","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Kullback–Leibler Divergence","metadata":{}},{"cell_type":"code","source":"# Define a custom KLLossLayer to compute the KL divergence loss\nclass KLLossLayer(tf.keras.layers.Layer):\n    def call(self, inputs):\n        mu, sigma = inputs  # Unpack the mean and log variance\n        kl_loss = 1 + sigma - tf.square(mu) - tf.math.exp(sigma)  # Compute the KL divergence\n        kl_loss = tf.reduce_mean(kl_loss) * -0.5  # Compute the mean KL divergence\n        self.add_loss(kl_loss)  # Add the KL divergence loss to the layer\n        return inputs  # Return the inputs","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Putting it all together","metadata":{}},{"cell_type":"code","source":"# Function to build the VAE model\ndef vae_model(encoder, decoder, input_shape):\n    inputs = tf.keras.layers.Input(shape=input_shape)  # Input layer with the given shape\n    mu, sigma, z = encoder(inputs)  # Get the outputs of the encoder\n    reconstructed = decoder(z)  # Get the reconstructed output from the decoder\n    kl_loss_layer = KLLossLayer()([mu, sigma])  # Compute the KL divergence loss\n    model = tf.keras.Model(inputs=inputs, outputs=reconstructed)  # Define the VAE model\n    return model  # Return the VAE model","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to get the encoder, decoder, and VAE models\ndef get_models(input_shape, latent_dim):\n    encoder, conv_shape = encoder_model(latent_dim=latent_dim, input_shape=input_shape)  # Build the encoder model\n    decoder = decoder_model(latent_dim=latent_dim, conv_shape=conv_shape)  # Build the decoder model\n    vae = vae_model(encoder, decoder, input_shape=input_shape)  # Build the VAE model\n    return encoder, decoder, vae  # Return the encoder, decoder, and VAE models","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get the encoder, decoder and 'master' model (called vae)\nencoder, decoder, vae = get_models(input_shape=(28, 28, 1), latent_dim=LATENT_DIM)  # Get the models with the specified input shape and latent dimensionality","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train the Model","metadata":{}},{"cell_type":"code","source":"optimizer = tf.keras.optimizers.Adam(learning_rate=0.002)\nloss_metric = tf.keras.metrics.Mean()\nmse_loss = tf.keras.losses.MeanSquaredError()\nbce_loss = tf.keras.losses.BinaryCrossentropy()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generate_and_save_images(model, epoch, step, test_input):\n  \"\"\"Helper function to plot our 16 images\n\n  Args:\n\n  model -- the decoder model\n  epoch -- current epoch number during training\n  step -- current step number during training\n  test_input -- random tensor with shape (16, LATENT_DIM)\n  \"\"\"\n  predictions = model.predict(test_input)\n\n  fig = plt.figure(figsize=(4,4))\n\n  for i in range(predictions.shape[0]):\n      plt.subplot(4, 4, i+1)\n      img = predictions[i, :, :, :] * 255\n      img = img.astype('int32')\n      plt.imshow(img)\n      plt.axis('off')\n\n  # tight_layout minimizes the overlap between 2 sub-plots\n  fig.suptitle(\"epoch: {}, step: {}\".format(epoch, step))\n  plt.savefig('image_at_epoch_{:04d}_step{:04d}.png'.format(epoch, step))\n  plt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Training loop. \n\n# generate random vector as test input to the decoder\nrandom_vector_for_generation = tf.random.normal(shape=[16, LATENT_DIM])\n\n# number of epochs\nepochs = 100\n\n# initialize the helper function to display outputs from an untrained model\ngenerate_and_save_images(decoder, 0, 0, random_vector_for_generation)\n\nfor epoch in range(epochs):\n  print('Start of epoch %d' % (epoch,))\n\n  # iterate over the batches of the dataset.\n  for step, x_batch_train in enumerate(train_dataset):\n    with tf.GradientTape() as tape:\n\n      # feed a batch to the VAE model\n      reconstructed = vae(x_batch_train)\n\n      # compute reconstruction loss\n      flattened_inputs = tf.reshape(x_batch_train, shape=[-1])\n      flattened_outputs = tf.reshape(reconstructed, shape=[-1])\n      loss = bce_loss(flattened_inputs, flattened_outputs) * 784\n      \n      # add KLD regularization loss\n      loss += sum(vae.losses)  \n\n    # get the gradients and update the weights\n    grads = tape.gradient(loss, vae.trainable_weights)\n    optimizer.apply_gradients(zip(grads, vae.trainable_weights))\n\n    # compute the loss metric\n    loss_metric(loss)\n\n    # display outputs every 100 steps\n    if step % 100 == 0:\n      display.clear_output(wait=False)    \n      generate_and_save_images(decoder, epoch, step, random_vector_for_generation)\n      print('Epoch: %s step: %s mean loss = %s' % (epoch, step, loss_metric.result().numpy()))","metadata":{},"execution_count":null,"outputs":[]}]}